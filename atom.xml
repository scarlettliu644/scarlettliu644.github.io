<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Scarlett Roland &amp; Robotic Vision</title>
  
  
  <link href="/scarlettliu644.github.io/atom.xml" rel="self"/>
  
  <link href="https://scarlettliu644.github.io/scarlettliu644.github.io/"/>
  <updated>2018-12-27T07:02:20.602Z</updated>
  <id>https://scarlettliu644.github.io/scarlettliu644.github.io/</id>
  
  <author>
    <name>Scarlett Liu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Topic Analysis of National Competition of Transport Science and Technology</title>
    <link href="https://scarlettliu644.github.io/scarlettliu644.github.io/2018/12/07/topicanalysis/"/>
    <id>https://scarlettliu644.github.io/scarlettliu644.github.io/2018/12/07/topicanalysis/</id>
    <published>2018-12-07T10:18:49.000Z</published>
    <updated>2018-12-27T07:02:20.602Z</updated>
    
    <content type="html"><![CDATA[<meta name="baidu-site-verification" content="BjioOSTsQ8"><p>Hi all. In this post, I’m going to introduce a standard scheme to do the data analysis. The object in this post analysis historical topics of <a href="http://www.nactrans.com.cn/index" target="_blank" rel="noopener">National Competition of Transport Science and Technology for Students</a>. This is a competition which is designed for UG students who are majoring in the Transportation Engineering.</p><h2 id="The-content-covered-by-this-post-are"><a href="#The-content-covered-by-this-post-are" class="headerlink" title="The content covered by this post are:"></a>The content covered by this post are:</h2><p>1&gt; description of OS<br>2&gt; setup of software and related packages<br>3&gt; the procedure of analysis</p><h3 id="Description-of-the-OS"><a href="#Description-of-the-OS" class="headerlink" title="Description of the OS"></a>Description of the OS</h3><p>Mine: MacBook Pro, OS version: Unix macOS Mojave, Processor: 3.1 GHz Intel Core i7, Memory: 16 GB<br>Python2.7 is preinstalled with Xcode in my MAC. Python2.7 is required for this project.</p><h3 id="Setup-of-environment-and-install-related-packages"><a href="#Setup-of-environment-and-install-related-packages" class="headerlink" title="Setup of environment and install related packages"></a>Setup of environment and install related packages</h3><ol><li>Install Anaconda: <a href="https://conda.io/docs/user-guide/install/macos.html" target="_blank" rel="noopener">Link</a></li><li>Install tools by terminal:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ pip install jieba <span class="comment"># for Chinese Analysis</span></span><br><span class="line">$ pip install pyldavis</span><br></pre></td></tr></table></figure></li></ol><h3 id="Procedure-of-analysis"><a href="#Procedure-of-analysis" class="headerlink" title="Procedure of analysis"></a>Procedure of analysis</h3><ol><li><p>Prepare your data. Here I used a summarized excel. Or you can srapy something from the website. I’ll talk about how to “scrapy” key information from a target website. Make sure your CSV file is encoded as “utf-8” format.<br>Now, assume you have an csv like this:</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/csv.png"></li><li><p>Open Terminal<br>Move (cd) to the folder where your excel is stored</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> Downloads</span><br><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><p>here is the snapshot if you successfully opened Jupyter Notebook:</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/jupyter.png"></li><li><p>Move to Jupyter Notebook, create a new Jupyter note, named “topic-analysis”, remember to choose python2.</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/jupyter2.png"><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd <span class="comment"># in order to process the excel, tool Pandas is needed</span></span><br><span class="line">df = pd.read_csv(<span class="string">"data-csu-organised.csv"</span>, encoding=<span class="string">'utf-8'</span>) <span class="comment"># read csv</span></span><br><span class="line">df.head() <span class="comment"># show data</span></span><br></pre></td></tr></table></figure><p>Check the number of row and column</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.shape</span><br></pre></td></tr></table></figure><p>If your CVS file is formatted correctly, you will see the output:</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/importdata.png"><p>Check Chinese words in the column of “Topic”</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">def chinese_word_cut(mytext):</span><br><span class="line">    <span class="built_in">return</span> <span class="string">" "</span>.join(jieba.cut(mytext))</span><br><span class="line">df[<span class="string">"content_cutted"</span>] = df.Topic.apply(chinese_word_cut)</span><br><span class="line">df.content_cutted.head() <span class="comment"># show data</span></span><br></pre></td></tr></table></figure><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/Chinese.png"><p>Vectorize the data</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer</span><br><span class="line">n_features = 200 <span class="comment"># set the number of key words</span></span><br><span class="line">tf_vectorizer = CountVectorizer(strip_accents = <span class="string">'unicode'</span>,</span><br><span class="line">                                max_features=n_features,</span><br><span class="line">                                stop_words=<span class="string">'english'</span>,</span><br><span class="line">                                max_df = 0.5,</span><br><span class="line">                                min_df = 10)</span><br><span class="line">tf = tf_vectorizer.fit_transform(df.content_cutted)</span><br></pre></td></tr></table></figure><p>You won’t see any outcome. Now applying an algorithm called “LDA” to find key topics</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import LatentDirichletAllocation</span><br><span class="line">n_topics = 6 <span class="comment"># set the number of topics for selected key words</span></span><br><span class="line">lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=50,</span><br><span class="line">                                learning_method=<span class="string">'online'</span>,</span><br><span class="line">                                learning_offset=50.,</span><br><span class="line">                                random_state=0)</span><br><span class="line">lda.fit(tf)</span><br></pre></td></tr></table></figure><p>Then you will see:</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/lda.png"><p>Define an function for printting each topic</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def print_top_words(model, feature_names, n_top_words):</span><br><span class="line">    <span class="keyword">for</span> topic_idx, topic <span class="keyword">in</span> enumerate(model.components_):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Topic #%d:"</span> % topic_idx)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">" "</span>.join([feature_names[i]</span><br><span class="line">                        <span class="keyword">for</span> i <span class="keyword">in</span> topic.argsort()[:-n_top_words - 1:-1]]))</span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">n_top_words = 15 <span class="comment"># say there are 5 key words for each topic</span></span><br><span class="line">tf_feature_names = tf_vectorizer.get_feature_names()</span><br><span class="line">print_top_words(lda, tf_feature_names, n_top_words) <span class="comment"># show</span></span><br></pre></td></tr></table></figure><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/topic1.png"><p>You can play with some parameters to get what you want:</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/topic2.png"><p>By this step, we can visualize the results in a nicer way.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import pyLDAvis</span><br><span class="line">import pyLDAvis.sklearn</span><br><span class="line">pyLDAvis.enable_notebook()</span><br><span class="line">pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)</span><br><span class="line">data = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)</span><br><span class="line">pyLDAvis.show(data)</span><br></pre></td></tr></table></figure><p>Here is the output:</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/output1.png"><p>When you move your mouse around you can see the part highlighted in red:</p><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/output2.png"><img src="/scarlettliu644.github.io/2018/12/07/topicanalysis/output3.png"><p>It clearly shows that which key words you should pay attention on your competition topic.</p></li></ol><p>Now you can interpret the data in your own way.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;meta name=&quot;baidu-site-verification&quot; content=&quot;BjioOSTsQ8&quot;&gt;

&lt;p&gt;Hi all. In this post, I’m going to introduce a standard scheme to do the data
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Welcome</title>
    <link href="https://scarlettliu644.github.io/scarlettliu644.github.io/2018/10/19/welcome/"/>
    <id>https://scarlettliu644.github.io/scarlettliu644.github.io/2018/10/19/welcome/</id>
    <published>2018-10-19T07:11:14.000Z</published>
    <updated>2018-12-27T07:02:23.171Z</updated>
    
    <content type="html"><![CDATA[<meta name="baidu-site-verification" content="BjioOSTsQ8"><p>Hi I’m Scarlett.</p><p>Welcome to <a href="https://srrv.com/" target="_blank" rel="noopener">SR<sup>2</sup>V</a>! Check <a href="https://srrv.com/projects" target="_blank" rel="noopener">Projects</a> for more info. If you are intrested in getting in touch, you can find the answer in <a href="https://srrv.com/contact" target="_blank" rel="noopener">Contact</a> or you can shoot me an email.</p><p>More is coming, please stay tuned.</p><p>If you want more info about SR<sup>2</sup>V. Please subscribe.</p><div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 50%;"><img src="/scarlettliu644.github.io/2018/10/19/welcome/im0.jpg"></div><div class="group-picture-column" style="width: 50%;"><img src="/scarlettliu644.github.io/2018/10/19/welcome/im1.jpg"></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="/scarlettliu644.github.io/2018/10/19/welcome/im2.jpg"></div></div></div></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;meta name=&quot;baidu-site-verification&quot; content=&quot;BjioOSTsQ8&quot;&gt;

&lt;p&gt;Hi I’m Scarlett.&lt;/p&gt;
&lt;p&gt;Welcome to &lt;a href=&quot;https://srrv.com/&quot; target=&quot;_blank
      
    
    </summary>
    
    
  </entry>
  
</feed>
