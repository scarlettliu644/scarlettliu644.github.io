{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Uni</th>\n",
       "      <th>Group</th>\n",
       "      <th>Year</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Host</th>\n",
       "      <th>Location</th>\n",
       "      <th>Theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>一等奖</td>\n",
       "      <td>基于多元数据融合的列车晚点预测系统</td>\n",
       "      <td>中南大学</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>13</td>\n",
       "      <td>长沙理工大学</td>\n",
       "      <td>湖南长沙</td>\n",
       "      <td>智慧与共享</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一等奖</td>\n",
       "      <td>基于信息补偿和视认纠偏的地下互通交织段交通安全导控系统</td>\n",
       "      <td>武汉理工大学</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>13</td>\n",
       "      <td>长沙理工大学</td>\n",
       "      <td>湖南长沙</td>\n",
       "      <td>智慧与共享</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>二等奖</td>\n",
       "      <td>基于蓝牙技术的共享单车规范化停车管理系统</td>\n",
       "      <td>长安大学</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>13</td>\n",
       "      <td>长沙理工大学</td>\n",
       "      <td>湖南长沙</td>\n",
       "      <td>智慧与共享</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>二等奖</td>\n",
       "      <td>交叉口“盲选车道”设计方法与仿真实现</td>\n",
       "      <td>长沙理工大学</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>13</td>\n",
       "      <td>长沙理工大学</td>\n",
       "      <td>湖南长沙</td>\n",
       "      <td>智慧与共享</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>二等奖</td>\n",
       "      <td>基于多传感器信息融合的智能停车系统</td>\n",
       "      <td>南京理工大学</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>13</td>\n",
       "      <td>长沙理工大学</td>\n",
       "      <td>湖南长沙</td>\n",
       "      <td>智慧与共享</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Level                        Topic     Uni Group  Year  Grade    Host  \\\n",
       "0   一等奖            基于多元数据融合的列车晚点预测系统    中南大学     A  2018     13  长沙理工大学   \n",
       "1   一等奖  基于信息补偿和视认纠偏的地下互通交织段交通安全导控系统  武汉理工大学     A  2018     13  长沙理工大学   \n",
       "2   二等奖         基于蓝牙技术的共享单车规范化停车管理系统    长安大学     A  2018     13  长沙理工大学   \n",
       "3   二等奖           交叉口“盲选车道”设计方法与仿真实现  长沙理工大学     A  2018     13  长沙理工大学   \n",
       "4   二等奖            基于多传感器信息融合的智能停车系统  南京理工大学     A  2018     13  长沙理工大学   \n",
       "\n",
       "  Location  Theme  \n",
       "0     湖南长沙  智慧与共享  \n",
       "1     湖南长沙  智慧与共享  \n",
       "2     湖南长沙  智慧与共享  \n",
       "3     湖南长沙  智慧与共享  \n",
       "4     湖南长沙  智慧与共享  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # in order to process the excel, tool Pandas is needed\n",
    "df = pd.read_csv(\"data-national-organised.csv\", encoding='utf-8') # read csv\n",
    "df.head() # show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/kz/yfyyt20x75z3_h5vc9tvmpxh0000gn/T/jieba.cache\n",
      "Loading model cost 0.446 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                   基于 多元 数据 融合 的 列车 晚点 预测 系统\n",
       "1    基于 信息 补偿 和 视认 纠偏 的 地下 互通 交织 段 交通安全 导 控系统\n",
       "2                基于 蓝牙 技术 的 共享 单车 规范化 停车 管理系统\n",
       "3                 交叉口 “ 盲选 车道 ” 设计 方法 与 仿真 实现\n",
       "4                   基于 多 传感器 信息 融合 的 智能 停车 系统\n",
       "Name: content_cutted, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "df[\"content_cutted\"] = df.Topic.apply(chinese_word_cut)\n",
    "df.content_cutted.head() # show data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "n_features = 200 # set the number of key words\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n",
       "             n_components=6, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 6 # set the number of topics for selected key words\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "设计 协同 研究 基于 优化 交通 公交 智能 技术 数据 系统 共享 车辆 仿真 装置\n",
      "()\n",
      "Topic #1:\n",
      "仿真 基于 数据 研究 优化 协同 设计 系统 车辆 智能 装置 技术 公交 交通 共享\n",
      "()\n",
      "Topic #2:\n",
      "技术 基于 设计 公交 仿真 系统 数据 协同 研究 装置 智能 交通 车辆 优化 共享\n",
      "()\n",
      "Topic #3:\n",
      "智能 装置 基于 公交 车辆 交通 共享 技术 设计 数据 研究 仿真 协同 系统 优化\n",
      "()\n",
      "Topic #4:\n",
      "系统 基于 设计 智能 车辆 交通 协同 研究 公交 技术 共享 数据 装置 仿真 优化\n",
      "()\n",
      "Topic #5:\n",
      "基于 共享 数据 车辆 技术 仿真 公交 研究 智能 交通 设计 系统 装置 协同 优化\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()\n",
    "    \n",
    "n_top_words = 15 # say there are 15 key words for each topic\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words) # show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "设计 基于 交通 研究 优化 智能 共享 协同 数据 车辆\n",
      "()\n",
      "Topic #1:\n",
      "协同 优化 研究 公交 基于 仿真 技术 设计 系统 交通\n",
      "()\n",
      "Topic #2:\n",
      "数据 仿真 研究 公交 共享 车辆 基于 设计 智能 优化\n",
      "()\n",
      "Topic #3:\n",
      "基于 技术 车辆 装置 智能 共享 公交 交通 数据 仿真\n",
      "()\n",
      "Topic #4:\n",
      "系统 基于 智能 公交 设计 共享 车辆 研究 交通 协同\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features = 150 # set the number of key words\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)\n",
    "n_topics = 5 # set the number of topics for selected key words\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "n_top_words = 10 # say there are 15 key words for each topic\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words) # show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [25/Dec/2018 19:23:19] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Dec/2018 19:23:19] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Dec/2018 19:23:19] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Dec/2018 19:23:19] \"GET /LDAvis.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Dec/2018 19:27:48] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Dec/2018 19:27:48] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Dec/2018 19:27:48] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Dec/2018 19:27:48] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "data = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.show(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100 # set the number of key words\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "n_topics = 3 # set the number of topics for selected key words\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf = tf_vectorizer.fit_transform(df.content_cutted)\n",
    "n_top_words = 4 # say there are 7 key words for each topic\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words) # show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
